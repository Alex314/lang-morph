{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de044f4d-3cdb-41aa-8305-b92c8b6e1096",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Sequence, Iterable\n",
    "import json\n",
    "from random import sample\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from analyser import (BaseAnalyser,\n",
    "                      punctuation_analyser,\n",
    "                      SpacingEntity, spacing_analyser,\n",
    "                      integer_analyser,\n",
    "                     )\n",
    "from dag import EntitiesDAG, BaseEntity, ConnectingEntity, TextEntity\n",
    "from tokenization import text_to_tokens\n",
    "from word_analyser import WordAnalyser, WordEntity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2f06a7b-34e1-47e2-8b7b-bad0be948f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_analyser = WordAnalyser.from_json('data/word_analyser.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23396e6c-5867-4f43-9558-b341be7a24be",
   "metadata": {},
   "source": [
    "# NameAnalyser implementation\n",
    "\n",
    "Having info about words which are parts of personal name, such are first names, surnames, patronimics, let's build analyser which would search for chains like that:\n",
    "```\n",
    "           first_name >> patronimic\n",
    "surname >> first_name >> patronimic\n",
    "           first_name >> patronimic >> surname\n",
    "surname >> first_name\n",
    "           first_name >> surname\n",
    "```\n",
    "And also for surnames with initials (for simplicity initials here are only single letter + period repeated twice)\n",
    "\n",
    "Analyser triggers on word and checks if there are chains defined by `LookupRule`'s. Each `LookupRule` contains info about required type of entity and guard function which takes entity and returns `None` if it doesn't match or numerical matching score if there's a match. For example, `surname_rule` returns score 2 if surname starts with capital letter and 1 for other surnames.<br>\n",
    "Then if full chain is found analyser creates new `PersonNameEntity` for it, which also contains likelihood (product of matching scores for matched entities).<br>\n",
    "Analyser's lookup implemented in a way that allows intermediate entities within the chain (for example spaces and connecting entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09aeda2d-6032-443e-ac8c-d9ffb32df068",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextBasedEntity(BaseEntity):\n",
    "    def __init__(self, text_content: str, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.features['text_content'] = text_content\n",
    "\n",
    "    @property\n",
    "    def text_content(self):\n",
    "        return self.features['text_content']\n",
    "\n",
    "class PersonNameEntity(TextBasedEntity):\n",
    "    def __init__(self, likelihood, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.features['likelihood'] = likelihood\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'Name<{self.text_content}|{self.features[\"likelihood\"]:.2g}>'\n",
    "\n",
    "\n",
    "def first_name_guard(entity: WordEntity):\n",
    "    if entity.features.get('person_name_part') == 'first_name':\n",
    "        if entity.text_content[0].isupper():\n",
    "            return 2\n",
    "        return 1\n",
    "\n",
    "def surname_guard(entity: WordEntity):\n",
    "    if entity.features.get('person_name_part') == 'surname':\n",
    "        if entity.text_content[0].isupper():\n",
    "            return 2\n",
    "        return 1\n",
    "\n",
    "def patronimic_guard(entity: WordEntity):\n",
    "    if entity.features.get('person_name_part') == 'patronimic':\n",
    "        if entity.text_content[0].isupper():\n",
    "            return 2\n",
    "        return 1\n",
    "\n",
    "def letter_guard(entity: TextEntity):\n",
    "    if len(entity.text_content) == 1 and entity.text_content.isalpha():\n",
    "        if entity.text_content[0].isupper():\n",
    "            return 2\n",
    "        return 1\n",
    "\n",
    "def period_guard(entity: TextEntity):\n",
    "    if entity.text_content == '.':\n",
    "        return 1\n",
    "\n",
    "\n",
    "class LookupRule:\n",
    "    def __init__(self, trigger_types, guard_func):\n",
    "        self.trigger_types = tuple(trigger_types)\n",
    "        self.guard_func = guard_func\n",
    "\n",
    "\n",
    "class PersonNameAnalyser(BaseAnalyser):\n",
    "    def __init__(self):\n",
    "        super().__init__(trigger_on_instances=[WordEntity])\n",
    "\n",
    "    def trigger(self, dag_entity: WordEntity):\n",
    "        patronimic_likelihood = patronimic_guard(dag_entity)\n",
    "        \n",
    "        surname_rule = LookupRule(trigger_types=[WordEntity], guard_func=surname_guard)\n",
    "        first_name_rule = LookupRule(trigger_types=[WordEntity], guard_func=first_name_guard)\n",
    "        letter_rule = LookupRule(trigger_types=[TextEntity], guard_func=letter_guard)\n",
    "        period_rule = LookupRule(trigger_types=[TextEntity], guard_func=period_guard)\n",
    "        \n",
    "        allowed_intermediate = (ConnectingEntity, SpacingEntity)\n",
    "        \n",
    "        if patronimic_likelihood is not None:\n",
    "            #            first_name >> patronimic\n",
    "            # surname >> first_name >> patronimic\n",
    "            #            first_name >> patronimic >> surname\n",
    "            for matched, likelihood in self.lookup(ref_entity=dag_entity,\n",
    "                                                   rules_chain=[first_name_rule],\n",
    "                                                   position='before',\n",
    "                                                   allowed_intermediate=allowed_intermediate):\n",
    "                # first_name >> patronimic\n",
    "                self.embed_result(matched, likelihood*patronimic_likelihood)\n",
    "                for matched2, likelihood2 in self.lookup(ref_entity=matched[0],\n",
    "                                                         rules_chain=[surname_rule],\n",
    "                                                         position='before',\n",
    "                                                         allowed_intermediate=allowed_intermediate):\n",
    "                    # surname >> first_name >> patronimic\n",
    "                    self.embed_result(matched2 + matched[1:],\n",
    "                                      likelihood2*likelihood*patronimic_likelihood)\n",
    "                for matched2, likelihood2 in self.lookup(ref_entity=dag_entity,\n",
    "                                                         rules_chain=[surname_rule],\n",
    "                                                         position='after',\n",
    "                                                         allowed_intermediate=allowed_intermediate):\n",
    "                    # first_name >> patronimic >> surname\n",
    "                    self.embed_result(matched + matched2[1:],\n",
    "                                      likelihood2*likelihood*patronimic_likelihood)\n",
    "        \n",
    "        surname_likelihood = surname_guard(dag_entity)\n",
    "        if surname_likelihood is not None:\n",
    "            initials_chain = [letter_rule, period_rule, letter_rule, period_rule]\n",
    "            # surname L.L.\n",
    "            for matched, likelihood in self.lookup(ref_entity=dag_entity,\n",
    "                                                   rules_chain=initials_chain,\n",
    "                                                   position='after',\n",
    "                                                   allowed_intermediate=allowed_intermediate):\n",
    "                self.embed_result(matched, likelihood*surname_likelihood)\n",
    "            # L.L. surname\n",
    "            for matched, likelihood in self.lookup(ref_entity=dag_entity,\n",
    "                                                   rules_chain=initials_chain,\n",
    "                                                   position='before',\n",
    "                                                   allowed_intermediate=allowed_intermediate):\n",
    "                self.embed_result(matched, likelihood*surname_likelihood)\n",
    "            # surname >> first_name\n",
    "            for matched, likelihood in self.lookup(ref_entity=dag_entity,\n",
    "                                                   rules_chain=[first_name_rule],\n",
    "                                                   position='after',\n",
    "                                                   allowed_intermediate=allowed_intermediate):\n",
    "                self.embed_result(matched, likelihood*surname_likelihood)\n",
    "            # first_name >> surname\n",
    "            for matched, likelihood in self.lookup(ref_entity=dag_entity,\n",
    "                                                   rules_chain=[first_name_rule],\n",
    "                                                   position='before',\n",
    "                                                   allowed_intermediate=allowed_intermediate):\n",
    "                self.embed_result(matched, likelihood*surname_likelihood)\n",
    "\n",
    "    def lookup(self, ref_entity: BaseEntity,\n",
    "               rules_chain: Iterable[LookupRule],  # TODO: fix Iterable is not reversable \n",
    "               position: str = 'after',\n",
    "               allowed_intermediate: tuple[BaseEntity] = (ConnectingEntity,)):\n",
    "        to_search = ref_entity.next_entities\n",
    "        if position == 'before':\n",
    "            to_search = ref_entity.previous_entities\n",
    "            rules_chain = reversed(rules_chain)\n",
    "        \n",
    "        to_check = [([ref_entity, i], rules_chain, 1) for i in to_search]\n",
    "        checked = {i for i in to_search}\n",
    "        while len(to_check) > 0:\n",
    "            (*head, entity), (rule, *rules_tail), cur_likelihood = to_check.pop(0)\n",
    "            if isinstance(entity, rule.trigger_types):\n",
    "                likelihood = rule.guard_func(entity)\n",
    "                if likelihood is not None:\n",
    "                    # Matched element of the chain\n",
    "                    cur_likelihood *= likelihood\n",
    "                    if len(rules_tail) == 0:\n",
    "                        # Reach end of the rules_chain\n",
    "                        if position == 'after':\n",
    "                            yield [*head, entity], cur_likelihood\n",
    "                        elif position == 'before':\n",
    "                            yield [entity, *head[::-1]], cur_likelihood\n",
    "                    else:\n",
    "                        to_search = entity.next_entities\n",
    "                        if position == 'before':\n",
    "                            to_search = entity.previous_entities\n",
    "                        for i in to_search:\n",
    "                            if not i in checked:\n",
    "                                checked.add(i)\n",
    "                            to_check.append(([*head, entity, i], rules_tail, cur_likelihood))\n",
    "            if isinstance(entity, allowed_intermediate):\n",
    "                to_search = entity.next_entities\n",
    "                if position == 'before':\n",
    "                    to_search = entity.previous_entities\n",
    "                for i in to_search:\n",
    "                    to_check.append(([*head, entity, i], [rule, *rules_tail], cur_likelihood))\n",
    "\n",
    "    def embed_result(self, matched_dag_entities: Sequence[BaseEntity], likelihood):\n",
    "        text = ''.join(i.text_content for i in matched_dag_entities\n",
    "                       if isinstance(i, (TextEntity, WordEntity, SpacingEntity)))\n",
    "        new_entity = PersonNameEntity(likelihood=likelihood, text_content=text)\n",
    "        new_entity.features['matched_entities'] = list(matched_dag_entities)\n",
    "        for i in matched_dag_entities[0].previous_entities:\n",
    "            i.add_next(new_entity)\n",
    "        for i in matched_dag_entities[-1].next_entities:\n",
    "            new_entity.add_next(i)\n",
    "        for i in matched_dag_entities:\n",
    "            i.part_of.append(new_entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2029dc1a-cbd7-4cea-a7df-3be8886ffc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_name_analyser = PersonNameAnalyser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c29db9d-3376-4b20-b322-d4c1fff115c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "•Мельниченко• •Иван• •Иванович•\n",
      "                    ␣ Word<Иванович|иванович>\n",
      "               Word<Иван|иван>\n",
      "             ␣ Name<Иван Иванович|4>\n",
      " Word<Мельниченко|мельниченко>\n",
      " Word<Мельниченко|мельниченко>\n",
      " Name<Мельниченко Иван Иванович|8>\n",
      " Name<Мельниченко Иван|4>\n"
     ]
    }
   ],
   "source": [
    "input_text = 'Мельниченко Иван Иванович'\n",
    "# input_text = 'Зубенко Михаил Петрович'\n",
    "# input_text = 'Михаил Петрович Зубенко'\n",
    "# input_text = 'Зубенко р.П. Мельниченко З.  П.'\n",
    "# input_text = '''Борейчук Максим Петрович'''\n",
    "tokens = text_to_tokens(input_text)\n",
    "dag = EntitiesDAG(tokens)\n",
    "spacing_analyser.analyse(dag)\n",
    "word_analyser.analyse(dag)\n",
    "person_name_analyser.analyse(dag)\n",
    "dag.pprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0da045-4d14-48ee-b828-8e91eba2baff",
   "metadata": {},
   "source": [
    "# Test on real-world data\n",
    "\n",
    "To test that idea generally works I've applied pipeline to some messages from public social media groups and printed matches of `PersonNameEntity`'s. Results shows that analyser matches personal names. From quik overview I'd say it gives low amount of false positives, but by design due to the limited vocabulary it gives quite a lot of false negatives.\n",
    "\n",
    "So far I haven't performed any numerical evaluations on available labeled datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ff2e1b6-2355-46b9-a6dc-5df8f96bac58",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/messages_list.json') as f:\n",
    "    messages_list = json.load(f)\n",
    "texts = [i['text'] for i in messages_list[:15_000]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0877a85-7126-47c8-856d-531d6c7df3a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['В вашем ассортименте топовое 8 общежитие \\nЗа 3 и 6 не шарю',\n",
       " 'Ти шо тут робиш',\n",
       " 'Спиш?',\n",
       " 'Слінченко, Слінченко!!!!',\n",
       " 'F']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample(texts, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c96b5435-2c89-4645-a2f0-a8527afa9473",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c4be2ebe4be4022b4161d897ab52d53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name<Тараса Шевченка|4>\n",
      "\n",
      "Name<Карла Маркса|4>\n",
      "\n",
      "Name<Карпенко А.М.|8>\n",
      "\n",
      "Name<Коваленко Максим|4>\n",
      "\n",
      "Name<Эдуард Петрович|4>\n",
      "Name<Эдуард Петрович|4>\n",
      "Name<Эдуард Петрович|4>\n",
      "\n",
      "Name<Роман Богданович|4>\n",
      "Name<Роман Богданович|4>\n",
      "\n",
      "Name<Паша Коваль|4>\n",
      "\n",
      "Name<Виталий Андреевич|4>\n",
      "\n",
      "Name<Степанов Денис|4>\n",
      "\n",
      "Name<Микола Тарасович|4>\n",
      "Name<Андрійчук Микола Тарасович|8>\n",
      "Name<Андрійчук Микола|4>\n",
      "\n",
      "Name<Гринь А.Р.|8>\n",
      "Name<Білоконь В.П.|8>\n",
      "\n",
      "Name<Пушик Максим|4>\n",
      "\n",
      "Name<Андрей Анатольевич|4>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "analysers = [spacing_analyser, word_analyser, person_name_analyser]\n",
    "\n",
    "for text in tqdm(texts):\n",
    "    tokens = text_to_tokens(text)\n",
    "    dag = EntitiesDAG(tokens)\n",
    "    for a in analysers:\n",
    "        a.analyse(dag)\n",
    "    \n",
    "    smth_printed = False\n",
    "    dag_printed = False\n",
    "    to_print = []\n",
    "    for i in dag:\n",
    "        if isinstance(i, PersonNameEntity):\n",
    "            if i.features['likelihood'] >= 4:\n",
    "                print(i)\n",
    "                smth_printed = True\n",
    "    if smth_printed:\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0708c9a-7a5e-49dd-b9fb-fac4b126ef2b",
   "metadata": {},
   "source": [
    "P.S. it prints captured names with likelihood scores of that match. Also by design even when capturing full name like \"Андрійчук Микола Тарасович\" it also captures parts of that name like \"Микола Тарасович\" and \"Андрійчук Микола\", that may be usefull if full capture is erroneous"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.9",
   "language": "python",
   "name": "python3.9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
