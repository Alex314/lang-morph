{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74e29de0-3d20-4677-8dec-2da5a626da1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "from collections.abc import Sequence, Iterable\n",
    "import json\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from tokenization import text_to_tokens\n",
    "from dag import EntitiesDAG, BaseEntity, ConnectingEntity, TextEntity\n",
    "from analyser import (BaseAnalyser, MatchAnalyser,\n",
    "                      punctuation_analyser,\n",
    "                      spacing_analyser,\n",
    "                      integer_analyser,\n",
    "                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162759cb-faf6-4bc2-9ad5-46057db65af6",
   "metadata": {},
   "source": [
    "# WordAnalyser\n",
    "\n",
    "Taking wordforms prepared in the notebook \"0_DataLoading\", let's build `WordAnalyser`, which would add `WordEntity` to the DAG for each token (or several consequent tokens) matched with existing wordform."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83efc3ff-f59e-455e-ace8-0600384be66f",
   "metadata": {},
   "source": [
    "## Loading Wordforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c8cfe8f-9e01-4c3e-968e-b117674511a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_wordworms(path='data'):\n",
    "    filenames = os.listdir(path)\n",
    "    wf_filenames = [i for i in filenames if i.endswith('wordforms.json')]\n",
    "    all_wordforms = []\n",
    "    for fn in wf_filenames:\n",
    "        filepath = os.path.join(path, fn)\n",
    "        with open(filepath) as f:\n",
    "            wforms = json.load(f)\n",
    "            assert isinstance(wforms, list)\n",
    "            print(f'{len(wforms)} from {fn}')\n",
    "            all_wordforms.extend(wforms)\n",
    "    return all_wordforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d989069f-3b71-46be-8f29-01d46b7d76e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4928901 from ukr_wordforms.json\n",
      "4449602 from ru_wordforms.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9378503"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_wordforms = load_wordworms()\n",
    "len(all_wordforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39669e4-33b1-441a-a15f-07b5d46aaa65",
   "metadata": {},
   "source": [
    "## Exploring\n",
    "\n",
    "Here it's presented how wordforms are stored and which fields does they contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb7b727d-fd09-4d85-b612-e9793266e5f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'wordform': 'а',\n",
       "  'main_form': 'а',\n",
       "  'mphdict_word_base_id': 0,\n",
       "  'mphdict_pos_name': 'вигук',\n",
       "  'lang': 'українська'},\n",
       " {'wordform': 'а',\n",
       "  'main_form': 'а',\n",
       "  'mphdict_word_base_id': 1,\n",
       "  'mphdict_pos_name': 'сполучник',\n",
       "  'lang': 'українська'},\n",
       " {'wordform': 'а',\n",
       "  'main_form': 'а',\n",
       "  'mphdict_word_base_id': 2,\n",
       "  'mphdict_pos_name': 'частка',\n",
       "  'lang': 'українська'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_wordforms[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4a2376c-0c0d-4894-9cb3-df9e5fab551b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 9378503/9378503 [00:10<00:00, 904334.75it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('wordform', 9378503),\n",
       " ('main_form', 9378503),\n",
       " ('lang', 9378503),\n",
       " ('mphdict_word_base_id', 4928901),\n",
       " ('mphdict_pos_name', 4928901),\n",
       " ('mphdict_gramm_category', 4916764),\n",
       " ('source', 4449602),\n",
       " ('odict_column', 4449602),\n",
       " ('odict_row', 4449602),\n",
       " ('odict_pos', 4449602),\n",
       " ('mphdict_field5', 730022),\n",
       " ('pos', 146587),\n",
       " ('person_name_part', 146587),\n",
       " ('case', 119617),\n",
       " ('case_ukr', 111697),\n",
       " ('father_name', 17255),\n",
       " ('case_ru', 7920)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many records have each feature filled:\n",
    "\n",
    "cnt = Counter()\n",
    "for i in tqdm(all_wordforms):\n",
    "    cnt.update(list(i))\n",
    "cnt.most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01a4061b-52b9-4915-9d00-66c8a539a476",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 9378503/9378503 [00:44<00:00, 209424.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mphdict_pos_name 51\n",
      "[('прикметник', 1751512), ('дієслово недоконаного виду', 573273), ('дієслово доконаного виду', 522705), ('дієприкметник', 464668), ('іменник чоловічого роду', 405010), ('іменник жіночого роду', 363454), ('іменник чоловічого роду, істота', 242205), ('іменник середнього роду', 208465), ('іменник жіночого роду, істота', 96076), ('прикметник, найвищий ступінь', 57661)]\n",
      "\n",
      "lang 2\n",
      "[('українська', 4928901), ('русский', 4449602)]\n",
      "\n",
      "mphdict_gramm_category 27\n",
      "[(6, 412314), (7, 307210), (3, 304728), (1, 291101), (4, 286495), (13, 268691), (10, 256157), (2, 253108), (5, 244636), (12, 215154)]\n",
      "\n",
      "pos 1\n",
      "[('proper_name', 146587)]\n",
      "\n",
      "person_name_part 3\n",
      "[('surname', 121338), ('patronimic', 17255), ('first_name', 7994)]\n",
      "\n",
      "case_ukr 7\n",
      "[('місцевий', 19461), ('кличний', 18588), ('давальний', 17211), ('родовий', 14118), ('знахідний', 14118), ('орудний', 14118), ('називний', 14083)]\n",
      "\n",
      "case 7\n",
      "[('locative', 20781), ('vocative', 18588), ('dative', 18531), ('genitive', 15438), ('accusative', 15438), ('instrumental', 15438), ('nominative', 15403)]\n",
      "\n",
      "source 1\n",
      "[('odict', 4449602)]\n",
      "\n",
      "odict_column 217\n",
      "[(0, 102091), (2, 99550), (3, 99550), (4, 99550), (5, 99550), (6, 99540), (7, 99540), (8, 99309), (9, 99006), (10, 98999)]\n",
      "\n",
      "odict_pos 24\n",
      "[('нсв', 1608062), ('св', 1170158), ('п', 747304), ('ж', 222016), ('м', 200381), ('св-нсв', 164561), ('мо', 148896), ('с', 89493), ('жо', 57147), ('мн.', 14435)]\n",
      "\n",
      "father_name 571\n",
      "[('анатолий', 58), ('антоний', 58), ('аркадий', 58), ('арсений', 58), ('афанасий', 58), ('валерий', 58), ('василий', 58), ('викарий', 58), ('виталий', 58), ('геннадий', 58)]\n",
      "\n",
      "case_ru 6\n",
      "[('именительный', 1320), ('родительный', 1320), ('дательный', 1320), ('винительный', 1320), ('творительный', 1320), ('предложный', 1320)]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Which unique values does features with low cardinality have:\n",
    "\n",
    "feature_counters = defaultdict(Counter)\n",
    "\n",
    "for i in tqdm(all_wordforms):\n",
    "    for k, v in i.items():\n",
    "        feature_counters[k].update([v])\n",
    "\n",
    "for k, v in feature_counters.items():\n",
    "    if len(v) > 1000:\n",
    "        continue\n",
    "    print(k, len(v))\n",
    "    print(v.most_common(10))\n",
    "    print()\n",
    "\n",
    "# Format is \"feature name\", \"cardinality\", (new line) \"top-10 values\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e798a9fe-e830-479a-b408-83036e75260c",
   "metadata": {},
   "source": [
    "## Preparing wordforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bbe2d7b-d269-4aac-b921-8556be6075a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 9378503/9378503 [00:09<00:00, 959385.46it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5215116"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dict wordform (as string) -> list of full wordform info for every matched wordform\n",
    "\n",
    "wf_to_matches_tmp = defaultdict(list)\n",
    "\n",
    "j = 0\n",
    "for i in tqdm(all_wordforms):\n",
    "    wf_to_matches_tmp[i['wordform']] += [i]\n",
    "len(wf_to_matches_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "878c9404-f289-4b32-bd0b-4b3ee9ced90a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 5215116/5215116 [00:29<00:00, 175813.56it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5215116"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reduce different cases into single record\n",
    "cols_to_ignore = {'mphdict_gramm_category', 'case_ukr', 'odict_column', 'case_ru'}\n",
    "\n",
    "wf_to_matches = {}\n",
    "for wf, matches in tqdm(wf_to_matches_tmp.items()):\n",
    "    multicases = defaultdict(set)\n",
    "    for i in matches:\n",
    "        i = {k:v for k, v in i.items() if k not in cols_to_ignore}\n",
    "        if not 'case' in i:\n",
    "            frozen = tuple(sorted(i.items()))\n",
    "            multicases[frozen] = set()\n",
    "            continue\n",
    "        case = None\n",
    "        frozen = []\n",
    "        for k, v in i.items():\n",
    "            if k == 'case':\n",
    "                case = v\n",
    "            elif k in cols_to_ignore:\n",
    "                pass\n",
    "            else:\n",
    "                frozen.append((k, v))\n",
    "        frozen = tuple(sorted(frozen))\n",
    "        multicases[frozen].add(case)\n",
    "    \n",
    "    matches = []\n",
    "    for k, v in multicases.items():\n",
    "        dct = dict(k)\n",
    "        if len(v) > 0:\n",
    "            dct['possible_cases'] = list(v)\n",
    "        matches.append(dct)\n",
    "    wf_to_matches[wf] = matches\n",
    "len(wf_to_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1816c8cf-4ba4-43fa-a8b7-cf22e274e539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 5215116 unique wordforms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['а', 'аахен', 'аахена']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_wfs = list(wf_to_matches)\n",
    "print(f'Total {len(unique_wfs)} unique wordforms')\n",
    "unique_wfs[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8534af66-7347-4106-8f33-9253b4ffb7e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████| 5215116/5215116 [00:46<00:00, 111214.00it/s]\n"
     ]
    }
   ],
   "source": [
    "# Tokenized wordforms (i.e. words with dash would consist of several tokens)\n",
    "\n",
    "tokenized_unique_wf = []\n",
    "for i in tqdm(unique_wfs):\n",
    "    tokenized_unique_wf.append(text_to_tokens(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb0bef86-674b-4691-b800-b8864c36df5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████| 5215116/5215116 [00:02<00:00, 2430544.17it/s]\n",
      "100%|████████████████████████████████████████████████| 5215116/5215116 [00:02<00:00, 2207117.10it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6026"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mapping of first token in a complex word to the tail of tokens\n",
    "\n",
    "start_to_continuations = defaultdict(list)\n",
    "for head, *tail in tqdm(tokenized_unique_wf):\n",
    "    if len(tail) == 0:\n",
    "        continue\n",
    "    start_to_continuations[head] += [tail]\n",
    "\n",
    "for head, *tail in tqdm(tokenized_unique_wf):\n",
    "    if len(tail) == 0 and head in start_to_continuations:\n",
    "        start_to_continuations[head] += [tail]\n",
    "len(start_to_continuations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "345fae17-3e29-4e7a-9f01-20fe09d1086f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['-', 'аби'], ['-', 'но'], ['-', 'то'], []]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example\n",
    "start_to_continuations['аби']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dfbc3fd-f35e-4518-89fc-d5880f7c9b22",
   "metadata": {},
   "source": [
    "## Building WordAnalyser\n",
    "\n",
    "`WordAnalyser` designed to match all possible wordforms for a given token. The problems with ambiguity between common and rare wordforms could be potentially solved later by assigning some scores to wordforms.\n",
    "\n",
    "**Implementation of `WordAnalyser`:**\n",
    "\n",
    "Analyser iterates over all entities in the DAG, and executes method `trigger` on those which are instances of `TextEntity`. Then it checks if there are wordforms which, when tokenized, starts like a given `TextEntity`. If there are matches of multi-token wordforms analyser also looks for corresponding chains of tokens in the graph (ommiting `ConnectingEntity` on it's way). For each full match new `WordEntity` created. New entity placed into DAG in a way, that all entities which have edges toward first matched token now points to that new entity too, and that entity points to all nodes toward which last matched token has edges (so new entiry becomes kinda parallel to matched sequence).<br>\n",
    "Example could be rendered as that:<br>\n",
    "Let's take DAG generated from string \"Count d'Artagnan\". It will consist of 11 entities (including `ConnectingEntity`)\n",
    "```\n",
    "•Count• •d•'•Artagnan•\n",
    "```\n",
    "And we'll match 2 words. Let's say first token \"Count\" was matched to two possible wordforms and chain of 3 tokens \"d->'->Artagnan\" matched to a single wordform. We will obtain such DAG:\n",
    "```\n",
    "•─Count───────────• •─d•'•Artagnan───•\n",
    "├Word<count(noun)>┤ └Word<d'artagnan>┘\n",
    "└Word<count(verb)>┘\n",
    "```\n",
    "Resulting DAG wil consist of 14 entities (3 new `WordEntity` added)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8851cf3-b963-46c7-9aa9-0fe3ed0e1777",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordEntity(BaseEntity):\n",
    "    def __init__(self, text_content: str, features, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.features |= features\n",
    "        self.features['text_content'] = text_content\n",
    "    \n",
    "    @property\n",
    "    def text_content(self):\n",
    "        return self.features['text_content']\n",
    "\n",
    "    def __str__(self):\n",
    "        main_form = self.features.get('main_form')\n",
    "        main_form = f'|{main_form}' if main_form else ''\n",
    "        return f'Word<{self.text_content}{main_form}>' #  *len(self.text_content)\n",
    "\n",
    "\n",
    "class WordAnalyser(BaseAnalyser):\n",
    "    def __init__(self,\n",
    "                 wordform_to_features,\n",
    "                 start_to_continuations,\n",
    "                 allow_intermediate_types: Iterable[BaseEntity] = (ConnectingEntity,)\n",
    "                ):\n",
    "        super().__init__(trigger_on_instances=[TextEntity])\n",
    "        self.wordform_to_features = wordform_to_features\n",
    "        self.start_to_continuations = start_to_continuations\n",
    "        self.allow_intermediate_types = tuple(allow_intermediate_types)\n",
    "\n",
    "    def trigger(self, dag_entity: TextEntity):\n",
    "        continuations = self.start_to_continuations.get(dag_entity.text_content.lower(), [[]])\n",
    "        \n",
    "        for next_tokens in continuations:\n",
    "            if len(next_tokens) == 0:  # Last entity of sequence matched\n",
    "                self.embed_result(matched_dag_entities=[dag_entity])\n",
    "                continue\n",
    "            self.match_sequence(dag_entity, next_tokens)\n",
    "    \n",
    "    def match_sequence(self, dag_entity: BaseEntity, next_tokens: Sequence[str], matched=None):\n",
    "        if matched is None:\n",
    "            matched = [dag_entity]\n",
    "        for i in dag_entity.next_entities:\n",
    "            if isinstance(i, TextEntity) and i.text_content.lower() == next_tokens[0]:\n",
    "                matched.append(i)\n",
    "                if len(next_tokens) == 1:\n",
    "                    self.embed_result(matched)\n",
    "                    continue\n",
    "                self.match_sequence(dag_entity=i, next_tokens=next_tokens[1:], matched=matched)\n",
    "            elif isinstance(i, self.allow_intermediate_types):  # Pass allowed entity\n",
    "                matched.append(i)\n",
    "                self.match_sequence(dag_entity=i, next_tokens=next_tokens, matched=matched)\n",
    "                \n",
    "    \n",
    "    def embed_result(self, matched_dag_entities: Sequence[BaseEntity]):\n",
    "        text = ''.join(i.text_content for i in matched_dag_entities if isinstance(i, TextEntity))\n",
    "        possible_features = self.wordform_to_features.get(text.lower(), [])\n",
    "        for features in possible_features:\n",
    "            new_entity = WordEntity(text_content=text, features=features)\n",
    "            for i in matched_dag_entities[0].previous_entities:\n",
    "                i.add_next(new_entity)\n",
    "            for i in matched_dag_entities[-1].next_entities:\n",
    "                new_entity.add_next(i)\n",
    "            for i in matched_dag_entities:\n",
    "                i.part_of.append(new_entity)\n",
    "    \n",
    "    def to_json(self, filepath: str, **json_kwargs):\n",
    "        data = {\n",
    "            'wordform_to_features': self.wordform_to_features,\n",
    "            'start_to_continuations': self.start_to_continuations,\n",
    "        }\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(data, f, **json_kwargs)\n",
    "    \n",
    "    @classmethod\n",
    "    def from_json(cls, filepath: str,\n",
    "                  allow_intermediate_types: Iterable[BaseEntity] = (ConnectingEntity,)):\n",
    "        with open(filepath) as f:\n",
    "            data = json.load(f)\n",
    "        return cls(**data, allow_intermediate_types=allow_intermediate_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98313000-b133-4a75-890e-4d88fb292e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_analyser = WordAnalyser(wordform_to_features=wf_to_matches,\n",
    "                             start_to_continuations=start_to_continuations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7897080b-9797-46b9-8ede-bbb78c7b1c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WordAnalyser may be saved locally as json file\n",
    "# and loaded later without wordforms postprocessing\n",
    "\n",
    "word_analyser.to_json('data/word_analyser.json', indent=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "97518861-5f21-4fec-86b5-ebace49a7cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# allowed intermediate entity types are not saved in json\n",
    "word_analyser = WordAnalyser.from_json('data/word_analyser.json',\n",
    "                                       allow_intermediate_types=(ConnectingEntity,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f54c7d5e-1da5-4e61-ba02-5deb4f4666ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "•Программа• •Microsoft• •Imagine• •Academy• •для• •КПИ• •им•.• •Игоря• •Сикорско\n",
      "--------------------------------------------------------------------------------\n",
      "го•.•.•.•\n"
     ]
    }
   ],
   "source": [
    "input_text = 'Программа Microsoft Imagine Academy для КПИ им. Игоря Сикорского...'\n",
    "tokens = text_to_tokens(input_text)\n",
    "dag = EntitiesDAG(tokens)\n",
    "dag.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "59a1b8d1-14be-4d1c-8b1a-249792eba660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "•Программа• •Microsoft• •Imagine• •Academy• •для• •КПИ• •им•.• •Игоря• •Сикорско\n",
      "                                                                                \n",
      "                                                                                \n",
      "                                                                                \n",
      "                                                                      ␣         \n",
      "                                                              ␣ Word<Игоря|игорь\n",
      "                                                       ␣    Punct<.>\n",
      "                                                 ␣ Word<КПИ|кпити>\n",
      "                                             Word<для|для>\n",
      "                                             Word<для|дляти>\n",
      "                                             Word<для|длить>\n",
      "           ␣           ␣         ␣         ␣ Word<для|для>\n",
      " Word<Программа|программа>\n",
      "--------------------------------------------------------------------------------\n",
      "го•.•.•.•\n",
      "       Punct<.>\n",
      "     Punct<.>\n",
      "   Punct<.>\n",
      "   Punct<...>\n",
      ">\n"
     ]
    }
   ],
   "source": [
    "spacing_analyser.analyse(dag)\n",
    "punctuation_analyser.analyse(dag)\n",
    "word_analyser.analyse(dag)\n",
    "\n",
    "dag.pprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "36ace937-6577-4004-bcd3-4c3483c5b435",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "•Дідусь•,• •той• •що• •атестував•,• •посміхнувся• •й• •спитав•:•\n",
      "                                                              Punct<:>\n",
      "                                                     ␣ Word<спитав|спитати>\n",
      "                                                   Word<й|й>\n",
      "                                                 ␣ Word<й|й>\n",
      "                                   ␣ Word<посміхнувся|посміхнутися>\n",
      "                                 Punct<,>\n",
      "                     ␣ Word<атестував|атестувати>\n",
      "                  Word<що|що>\n",
      "                  Word<що|що>\n",
      "                ␣ Word<що|що>\n",
      "            Word<той|той>\n",
      "            Word<той|тоя>\n",
      "          ␣ Word<той|той>\n",
      "        Punct<,>\n",
      " Word<Дідусь|дідусь>\n"
     ]
    }
   ],
   "source": [
    "input_text = 'Дідусь, той що атестував, посміхнувся й спитав:'\n",
    "tokens = text_to_tokens(input_text)\n",
    "dag = EntitiesDAG(tokens)\n",
    "\n",
    "spacing_analyser.analyse(dag)\n",
    "punctuation_analyser.analyse(dag)\n",
    "word_analyser.analyse(dag)\n",
    "\n",
    "dag.pprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd703204-b694-40cd-b99e-20a6ec4e1c4c",
   "metadata": {},
   "source": [
    "# NormalizeAnalyzer\n",
    "\n",
    "Some symbols in the text could be written in a multiple ways: notably, there are multiple different dash symbols, apostrophe could be written by different symbols, etc.\n",
    "\n",
    "Let's normalize some of them as part of the analysis pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "47276757-31e3-46a2-b85e-96ee39d755f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "•Всі• •вони• •будуть• •безпосередньо• •пов•’•язані• •з•\n",
      "                                                     Word<з|з>\n",
      "                       Word<безпосередньо|безпосередньо>\n",
      "              Word<будуть|бути>\n",
      "       Word<вони|вон>\n",
      "       Word<вони|вони>\n",
      "       Word<вони|вонь>\n",
      " Word<Всі|ввесь>\n",
      " Word<Всі|весь>\n"
     ]
    }
   ],
   "source": [
    "input_text = 'Всі вони будуть безпосередньо пов’язані з'\n",
    "tokens = text_to_tokens(input_text)\n",
    "dag = EntitiesDAG(tokens)\n",
    "word_analyser.analyse(dag)\n",
    "dag.pprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d09420a-4c6c-4738-baec-659216c60e8c",
   "metadata": {},
   "source": [
    "Currently word \"пов’язані\" is not recognized, even tho it is present in the dictionary. The reason is - difference in apostrophe writing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "78cbde9b-e9d5-4905-957b-01b7e06311d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalization = {\n",
    "    '’': \"'\",\n",
    "    '–': '-',\n",
    "    '—': '-',\n",
    "}\n",
    "\n",
    "def normalization_factory(matched_entities: list[BaseEntity]):\n",
    "    text = ''.join(i.features.get('text_content', '') for i in matched_entities)\n",
    "    normalized_text = normalization[text]\n",
    "    new_entity = TextEntity(text_content=normalized_text)\n",
    "    new_entity.features['normalized_from'] = text\n",
    "    return new_entity\n",
    "\n",
    "valid_normalization_sequences = [\n",
    "    *([TextEntity(i)] for i in list(normalization)),\n",
    "]\n",
    "\n",
    "normalize_analyser = MatchAnalyser(valid_entity_sequences=valid_normalization_sequences,\n",
    "                                   match_entity_factory=normalization_factory,\n",
    "                                   trigger_on_instances=[TextEntity])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c88bba41-2498-41ab-97a0-3de0481585a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "•Всі• •вони• •будуть• •безпосередньо• •пов•’•язані• •з•\n",
      "                                           '         Word<з|з>\n",
      "                                       Word<пов'язані|пов'язаний>\n",
      "                       Word<безпосередньо|безпосередньо>\n",
      "              Word<будуть|бути>\n",
      "       Word<вони|вон>\n",
      "       Word<вони|вони>\n",
      "       Word<вони|вонь>\n",
      " Word<Всі|ввесь>\n",
      " Word<Всі|весь>\n"
     ]
    }
   ],
   "source": [
    "input_text = 'Всі вони будуть безпосередньо пов’язані з'\n",
    "tokens = text_to_tokens(input_text)\n",
    "dag = EntitiesDAG(tokens)\n",
    "normalize_analyser.analyse(dag)\n",
    "word_analyser.analyse(dag)\n",
    "dag.pprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6051f3bf-f783-4d53-8cbf-9d945ffd29db",
   "metadata": {},
   "source": [
    "Here `normalize_analyser` created new `TextEntiry` with normalized apostrophe, so now there is path in the DAG \"пов->'->язані\" (as well as old one \"пов->`->язані\" with backtick) and the word is recognized by the same word_analyser"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.9",
   "language": "python",
   "name": "python3.9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
